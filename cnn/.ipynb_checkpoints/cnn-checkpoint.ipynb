{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. User level:\n",
    "Run-time - p.cpu_times()\n",
    "\n",
    "II. Sytem level:\n",
    "1. CPU usage (p.cpu_percent())\n",
    "2. Memory usage (p.memory_percent())\n",
    "3. Hard drive usage\n",
    "4. RSS: Resident set size: This is non-swapped physical memory that a process had used\n",
    "5. VMS: Virtual memory size: This is the total virtual memory used by the process\n",
    "6. Number of page faults\n",
    "\n",
    "\n",
    "- **rss**: aka “Resident Set Size”, this is the non-swapped physical memory a process has used. On UNIX it matches “top“‘s RES column). On Windows this is an alias for wset field and it matches “Mem Usage” column of taskmgr.exe.\n",
    "- **vms**: aka “Virtual Memory Size”, this is the total amount of virtual memory used by the process. On UNIX it matches “top“‘s VIRT column. On Windows this is an alias for pagefile field and it matches “Mem Usage” “VM Size” column of taskmgr.exe.\n",
    "- **shared**: (Linux) memory that could be potentially shared with other processes. This matches “top“‘s SHR column).\n",
    "- **text** (Linux, BSD): aka TRS (text resident set) the amount of memory devoted to executable code. This matches “top“‘s CODE column).\n",
    "- **data** (Linux, BSD): aka DRS (data resident set) the amount of physical memory devoted to other than executable code. It matches “top“‘s DATA column).\n",
    "- **lib** (Linux): the memory used by shared libraries.\n",
    "- **dirty** (Linux): the number of dirty pages.\n",
    "- **pfaults** (macOS): number of page faults.\n",
    "- **pageins** (macOS): number of actual pageins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling utilities\n",
    "import psutil\n",
    "import shutil\n",
    "p = psutil.Process()\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# data processing\n",
    "typeList = [\n",
    "        'outlyingScatterPlot',\n",
    "        'skewedScatterPlot',\n",
    "        'clumpyScatterPlot',\n",
    "        'sparsedScatterPlot',\n",
    "        'striatedScatterPlot',\n",
    "        'convexScatterPlot',\n",
    "        'skinnyScatterPlot',\n",
    "        'stringyScatterPlot',\n",
    "        'monotonicScatterPlot']\n",
    "\n",
    "with open('../data/ScagnosticsTypicalData2.json') as f:\n",
    "    typicalData2 = json.load(f)\n",
    "\n",
    "with open('../data/RealWorldData10.json') as f:\n",
    "    realWorldData10 = json.load(f)\n",
    "    \n",
    "numPoints = 0 # minimum number of bins\n",
    "X_real10 = []\n",
    "y_real10 = []\n",
    "for ds in realWorldData10:\n",
    "    for d in ds:\n",
    "        if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n",
    "            X_real10.append(d['rectangularBins'])\n",
    "            y_real10.append(d['scagnostics'])\n",
    "\n",
    "X_typical2 = []\n",
    "y_typical2 = []\n",
    "y_typical_label2 = []\n",
    "for ds in typicalData2:\n",
    "    for d in ds:\n",
    "        # filter out invalid data\n",
    "        if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n",
    "            X_typical2.append(d['rectangularBins'])\n",
    "            y_typical2.append(d['scagnostics'])\n",
    "            y_typical_label2.append([1 if tl == d['dataSource'] else 0 for tl in typeList])\n",
    "\n",
    "X_real10 = np.array(X_real10)\n",
    "y_real10 = np.array(y_real10)\n",
    "\n",
    "X_typical2 = np.array(X_typical2)\n",
    "y_typical2 = np.array(y_typical2)\n",
    "\n",
    "X = np.concatenate([X_real10, X_typical2])\n",
    "y = np.concatenate([y_real10, y_typical2])\n",
    "# Re-shape the data\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "# # training part\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import Dropout\n",
    "# from keras.utils import plot_model\n",
    "# from keras.regularizers import l2\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.layers import BatchNormalization\n",
    "\n",
    "# # learned several reasons from here: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "# def create_cnn_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(40, 40, 1)))\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(Dropout(0.1))\n",
    "#     opt = SGD(lr=0.001, momentum=0.9)\n",
    "#     model.add(Dense(9, activation='relu'))\n",
    "#     # compile model\n",
    "#     model.compile(optimizer=opt, loss='mse')\n",
    "#     return model\n",
    "\n",
    "# model = create_cnn_model()\n",
    "\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
    "\n",
    "# model.fit(X, y, validation_split=0.33, epochs=500, callbacks=[mc, es])\n",
    "\n",
    "# load model\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "# Setting data\n",
    "num_runs = 31\n",
    "all_rows = num_runs + 1\n",
    "# cpu_percent\n",
    "cpu_percent = np.zeros(all_rows)\n",
    "# cpu_times\n",
    "cpu_times_user = np.zeros(all_rows)\n",
    "cpu_times_system = np.zeros(all_rows)\n",
    "cpu_times_children_user= np.zeros(all_rows)\n",
    "cpu_times_children_system = np.zeros(all_rows)\n",
    "# memory_percent\n",
    "memory_percent = np.zeros(all_rows)\n",
    "# disk_usage\n",
    "disk_usage_total = np.zeros(all_rows)\n",
    "disk_usage_used = np.zeros(all_rows)\n",
    "disk_usage_free = np.zeros(all_rows)\n",
    "# memory_full_info\n",
    "memory_full_info_rss = np.zeros(all_rows)\n",
    "memory_full_info_vms = np.zeros(all_rows)\n",
    "memory_full_info_pfaults = np.zeros(all_rows)\n",
    "memory_full_info_pageins = np.zeros(all_rows)\n",
    "memory_full_info_uss = np.zeros(all_rows)\n",
    "def set_data(p, i):\n",
    "    # cpu_percent\n",
    "    cpu_percent[i] = p.cpu_percent()\n",
    "    # cpu_time\n",
    "    cpu_times = p.cpu_times()\n",
    "    cpu_times_user[i] = cpu_times.user\n",
    "    cpu_times_system[i] = cpu_times.system\n",
    "    cpu_times_children_user[i] = cpu_times.children_user\n",
    "    cpu_times_children_system[i] = cpu_times.children_system\n",
    "    # memory_percent\n",
    "    memory_percent[i] = p.memory_percent()\n",
    "    # disk_usage\n",
    "    disk_usage = shutil.disk_usage('./')\n",
    "    disk_usage_total[i] = disk_usage.total \n",
    "    disk_usage_used[i] = disk_usage.used\n",
    "    disk_usage_free[i] = disk_usage.free\n",
    "    # memory_full_info\n",
    "    memory_full_info = p.memory_full_info()\n",
    "    memory_full_info_rss[i] = memory_full_info.rss\n",
    "    memory_full_info_vms[i] = memory_full_info.vms\n",
    "    memory_full_info_pfaults[i] = memory_full_info.pfaults\n",
    "    memory_full_info_pageins[i] = memory_full_info.pageins\n",
    "    memory_full_info_uss[i] = memory_full_info.uss\n",
    " \n",
    "# run for several times\n",
    "for run_idx in range(num_runs):\n",
    "    y_hat = model.predict(X)\n",
    "    set_data(p, run_idx)\n",
    "    \n",
    "# add last one out of the loop\n",
    "set_data(p, num_runs) \n",
    "\n",
    "# make data frame\n",
    "df = pd.DataFrame()\n",
    "# cpu_percent\n",
    "df['cpu_percent'] = cpu_percent\n",
    "# cpu_times\n",
    "df['cpu_times_user'] = cpu_times_user\n",
    "df['cpu_times_system'] = cpu_times_system\n",
    "df['cpu_times_children_user'] = cpu_times_children_user\n",
    "df['cpu_times_children_system'] = cpu_times_children_system\n",
    "# memory_percent\n",
    "df['memory_percent'] = memory_percent\n",
    "# disk_usage\n",
    "df['disk_usage_total'] = disk_usage_total\n",
    "df['disk_usage_used'] = disk_usage_used\n",
    "df['disk_usage_free'] = disk_usage_free\n",
    "# memory_full_info\n",
    "df['memory_full_info_rss'] = memory_full_info_rss\n",
    "df['memory_full_info_vms'] = memory_full_info_vms\n",
    "df['memory_full_info_pfaults'] = memory_full_info_pfaults\n",
    "df['memory_full_info_pageins'] = memory_full_info_pageins\n",
    "df['memory_full_info_uss'] = memory_full_info_uss\n",
    "\n",
    "# save file\n",
    "import calendar\n",
    "import time\n",
    "df.to_csv(f'cnn-{str(calendar.timegm(time.gmtime()))}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
